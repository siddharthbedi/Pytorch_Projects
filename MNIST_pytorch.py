# -*- coding: utf-8 -*-
"""Udacity- Epochs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Ff95qUAPr3W_jTYvoGws88N8w6YQXpu
"""

import torch
from torch import nn
import torch.nn.functional as F
from torchvision import datasets, transforms 
from torch import optim

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ),(0.5 ,)),])

trainset = datasets.MNIST('-/.pytorch/MNIST_data/' , download = True , train = 'True' , transform = transform)

#now in order to load training data we will use trainloader which will load packets of training data for us
trainloader = torch.utils.data.DataLoader(trainset, shuffle ='True' , batch_size = 64)

#building a forward feed network
model = nn.Sequential(nn.Linear(784,128), nn.ReLU(), nn.Linear(128,64), nn.ReLU() , nn.Linear(64,10), nn.LogSoftmax(dim = 1))

#determining the loss
#criterion = nn.CrossEntropyLoss()

criterion = nn.NLLLoss()



optimiser = optim.SGD(model.parameters() , lr = 0.01)

#print('Initial weights : ' , model[0].weight)

#importing out data
epoch = 20
loss = 0
for m in range(epoch):
  
  images, labels = next(iter(trainloader))
  images.resize_(64,784)
  optimiser.zero_grad()
  output = model.forward(images)
  loss = criterion(output, labels)
  print(m)
  

  loss.backward()
  
  optimiser.step()
print(loss)







#print('gradient of model without backprop : ' , model[0].weight.grad)



#print('weights of the model after backprop : ' , model[0].weight)